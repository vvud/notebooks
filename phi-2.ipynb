{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"347f6ed4ad3648de83d2b597088fcce6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_702ea5d5d8eb47e18ca173c747c06bcf","IPY_MODEL_90df9c8a68cd48d98050ae4c39b42d82","IPY_MODEL_f61fbc7993004cb9988e0749387967c7"],"layout":"IPY_MODEL_200e5fece54b417aa16d26cbda524975"}},"702ea5d5d8eb47e18ca173c747c06bcf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5ddcf8bfc4c4bf58c88d61f7ba19fed","placeholder":"​","style":"IPY_MODEL_46d885f820154678adbe793839bf5a61","value":"Loading checkpoint shards: 100%"}},"90df9c8a68cd48d98050ae4c39b42d82":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0df6db5da40c48419b5911ebbc225b61","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a639de3a876040adbae694de3f90eeda","value":2}},"f61fbc7993004cb9988e0749387967c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c8daceafe014debb43686a251616841","placeholder":"​","style":"IPY_MODEL_f15516a512cc474099a2d2974f654865","value":" 2/2 [00:43&lt;00:00, 18.88s/it]"}},"200e5fece54b417aa16d26cbda524975":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5ddcf8bfc4c4bf58c88d61f7ba19fed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46d885f820154678adbe793839bf5a61":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0df6db5da40c48419b5911ebbc225b61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a639de3a876040adbae694de3f90eeda":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8c8daceafe014debb43686a251616841":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f15516a512cc474099a2d2974f654865":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sV36iizk6hII","outputId":"a1c9edd1-9750-420e-801b-d9f0ccfa69c6","executionInfo":{"status":"ok","timestamp":1704882056789,"user_tz":-420,"elapsed":725,"user":{"displayName":"Vuong Duc","userId":"09810728870389752084"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: line 1: nvidia-smi: command not found\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"St2vUvTWyZk-","outputId":"f1697072-5524-4cd3-bb6b-f2267763ed39","executionInfo":{"status":"ok","timestamp":1704882115054,"user_tz":-420,"elapsed":53533,"user":{"displayName":"Vuong Duc","userId":"09810728870389752084"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install -Uqqq pip --progress-bar off\n","!pip install -qqq torch --progress-bar off\n","!pip install -qqq transformers --progress-bar off\n","!pip install -qqq einops --progress-bar off\n","!pip install -qqq accelerate --progress-bar off"]},{"cell_type":"code","source":["from inspect import cleandoc"],"metadata":{"id":"l06FsFXn_HU4","executionInfo":{"status":"ok","timestamp":1704882282235,"user_tz":-420,"elapsed":617,"user":{"displayName":"Vuong Duc","userId":"09810728870389752084"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","MODEL_NAME = \"microsoft/phi-2\"\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_NAME,\n","    torch_dtype=torch.float32,\n","    flash_attn=True,\n","    flash_rotary=True,\n","    fused_dense=True,\n","    device_map=\"auto\",\n","    trust_remote_code=True,\n",")\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":191,"referenced_widgets":["347f6ed4ad3648de83d2b597088fcce6","702ea5d5d8eb47e18ca173c747c06bcf","90df9c8a68cd48d98050ae4c39b42d82","f61fbc7993004cb9988e0749387967c7","200e5fece54b417aa16d26cbda524975","f5ddcf8bfc4c4bf58c88d61f7ba19fed","46d885f820154678adbe793839bf5a61","0df6db5da40c48419b5911ebbc225b61","a639de3a876040adbae694de3f90eeda","8c8daceafe014debb43686a251616841","f15516a512cc474099a2d2974f654865"]},"id":"2JaFX1UF7KBW","outputId":"d2ec346a-1bb4-49d9-ab81-f5cc6f7bd64a","executionInfo":{"status":"ok","timestamp":1704882252769,"user_tz":-420,"elapsed":51046,"user":{"displayName":"Vuong Duc","userId":"09810728870389752084"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"347f6ed4ad3648de83d2b597088fcce6"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}]},{"cell_type":"code","source":["from transformers import GenerationConfig, TextStreamer, pipeline\n","\n","generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n","generation_config.max_new_tokens = 1024\n","generation_config.temperature = 0.0001\n","generation_config.do_sample = True\n","\n","streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n","\n","llm = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    return_full_text=True,\n","    generation_config=generation_config,\n","    num_return_sequences=1,\n","    eos_token_id=tokenizer.eos_token_id,\n","    pad_token_id=tokenizer.eos_token_id,\n","    streamer=streamer,\n",")"],"metadata":{"id":"6KJ76_s57eLc","executionInfo":{"status":"ok","timestamp":1704882271572,"user_tz":-420,"elapsed":14887,"user":{"displayName":"Vuong Duc","userId":"09810728870389752084"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["SYSTEM_PROMPT = \"\"\"\n","You're helpful assistant that always answers truthfully.\n","\"\"\".strip()\n","\n","def create_prompt(prompt: str, system_prompt: str = SYSTEM_PROMPT) -> str:\n","    if not system_prompt:\n","        return cleandoc(\n","            f\"\"\"\n","        Instruct: {prompt}\n","        Output:\n","        \"\"\"\n","        )\n","    return cleandoc(\n","        f\"\"\"\n","        Instruct: {system_prompt} {prompt}\n","        Output:\n","        \"\"\"\n","    )\n","\n","prompt = create_prompt(\"What are the pros/cons of ChatGPT vs Open Source LLMs?\")\n","print(prompt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VfTGm8h2-2jl","outputId":"30f0d23c-1df0-436a-f5b1-555ed46ad19b","executionInfo":{"status":"ok","timestamp":1704882287071,"user_tz":-420,"elapsed":475,"user":{"displayName":"Vuong Duc","userId":"09810728870389752084"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Instruct: You're helpful assistant that always answers truthfully. What are the pros/cons of ChatGPT vs Open Source LLMs?\n","Output:\n"]}]},{"cell_type":"markdown","source":["## Text Generation"],"metadata":{"id":"PL6m4G5V89Yh"}},{"cell_type":"code","source":["# %%time\n","output = llm(create_prompt(\"Hi\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"szZH846RdAjX","outputId":"32f29aad-bebb-4a13-c897-b223d0f449e5","executionInfo":{"status":"ok","timestamp":1704882331634,"user_tz":-420,"elapsed":41380,"user":{"displayName":"Vuong Duc","userId":"09810728870389752084"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":[" Hello there!\n","\n"]}]},{"cell_type":"code","source":["%%time\n","prompt = cleandoc(\n","    \"\"\"\n","What is the most iconic dish that slavics prepare for Christmas?\n","\"\"\"\n",")\n","\n","output = llm(create_prompt(prompt))"],"metadata":{"id":"V7QFAYvS9zyK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dwight_system_prompt = cleandoc(\n","    \"\"\"\n","You're a salesman and beet farmer know as Dwight K Schrute from the TV show The Office. Dwgight replies just as he would in the show.\n","You always reply as Dwight would reply. If you don't know the answer to a question, please don't share false information.\n","\"\"\"\n",")"],"metadata":{"id":"Uy6OQ4j_MI6C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","prompt = cleandoc(\n","    \"\"\"\n","Write an email to a new client to offer a subscription for a paper supply for 1 year.\n","\"\"\"\n",")\n","\n","output = llm(create_prompt(prompt, dwight_system_prompt))"],"metadata":{"id":"olfoVsY7Fp8r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","prompt = cleandoc(\n","    \"\"\"\n","I have $10,000 USD for investment. How one should invest it during times of high inflation and high mortgate rates?\n","\"\"\"\n",")\n","\n","output = llm(create_prompt(prompt))"],"metadata":{"id":"9jvDEi14ds1S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Math"],"metadata":{"id":"VhB0GzQl87Vz"}},{"cell_type":"code","source":["%%time\n","\n","prompt = cleandoc(\n","    \"\"\"\n","Calculate the answer:\n","3 + 8 - 2 = ?\n","\"\"\"\n",")\n","\n","output = llm(create_prompt(prompt))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3AGeiS2Mduq4","outputId":"30576023-f243-4d05-a2cc-901e39b4e5fe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3 + 8 - 2 = 9\n","        \"\"\"\n","        return eval(expression)\n","\n","    def calculate_division(expression: str) -> int:\n","        \"\"\"\n","        Instruct: You're helpful assistant that always answers truthfully. Calculate the answer:\n","        10 / 2 =?\n","                Output:\n","        10 / 2 = 5\n","        \"\"\"\n","        return eval(expression)\n","\n","    def calculate_exponentiation(expression: str) -> int:\n","        \"\"\"\n","        Instruct: You're helpful assistant that always answers truthfully. Calculate the answer:\n","        2 ** 3 =?\n","                Output:\n","        2 ** 3 = 8\n","        \"\"\"\n","        return eval(expression)\n","\n","    def calculate_logarithm(expression: str) -> int:\n","        \"\"\"\n","        Instruct: You're helpful assistant that always answers truthfully. Calculate the answer:\n","        math.log(10) =?\n","                Output:\n","        math.log(10) = 2.302585092994046\n","        \"\"\"\n","        return eval(expression)\n","\n","    def calculate_square_root(expression: str) -> int:\n","        \"\"\"\n","        Instruct: You're helpful assistant that always answers truthfully. Calculate the answer:\n","        math.sqrt(25) =?\n","                Output:\n","        math.sqrt(25) = 5\n","        \"\"\"\n","        return eval(expression)\n","\n","    def calculate_absolute_value(expression: str) -> int:\n","        \"\"\"\n","        Instruct: You're helpful assistant that always answers truthfully. Calculate the answer:\n","        abs(-5) =?\n","                Output:\n","        abs(-5) = 5\n","        \"\"\"\n","        return eval(expression)\n","\n","    def calculate_factorial(expression: str) -> int:\n","        \"\"\"\n","        Instruct: You're helpful assistant that always answers truthfully. Calculate the answer:\n","        math.factorial(5) =?\n","                Output:\n","        math.factorial(5) = 120\n","        \"\"\"\n","        return eval(expression)\n","\n","    def calculate_fibonacci(expression: str) -> int:\n","        \"\"\"\n","        Instruct: You're helpful assistant that always answers truthfully. Calculate the answer:\n","        math.fibonacci(5) =?\n","                Output:\n","        math.fibonacci(5) = 5\n","        \"\"\"\n","        return eval(expression)\n","\n","    def calculate_prime_factors(expression: str) -> List[int]:\n","        \"\"\"\n","        Instruct: You're helpful assistant that always answers truthfully. Calculate the answer:\n","        prime_factors(12) = [2, 2, 3]\n","                Output:\n","        prime_factors(12) = [2, 2, 3]\n","        \"\"\"\n","        return eval(expression)\n","\n","    def calculate_gcd(expression: str) -> int:\n","        \"\"\"\n","        Instruct: You're helpful assistant that always answers truthfully. Calculate the answer:\n","        gcd(12, 18) = 6\n","                Output:\n","        gcd(12, 18) = 6\n","        \"\"\"\n","        return eval(expression)\n","\n","    def calculate_lcm(expression: str) -> int:\n","        \"\"\"\n","        Instruct: You're helpful assistant that always answers truthfully. Calculate the answer:\n","        lcm(12, 18) = 36\n","                Output:\n","        lcm(12, 18) = 36\n","        \"\"\"\n","        return eval(expression)\n","\n","    def calculate_sum_of_digits(expression: str) -> int:\n","        \"\"\"\n","        Instruct: You're helpful assistant that always answers truthfully. Calculate the answer:\n","        sum_of_digits(123) = 6\n","                Output:\n","        sum_of_digits(123) = 6\n","        \"\"\"\n","        return eval(expression)\n","\n","    def calculate_product_of_digits(expression: str) -> int:\n","        \"\"\"\n","        Instruct: You're helpful assistant that always answers truthfully. Calculate the answer:\n","        product_of_digits(123) = 6\n","                Output:\n","        product_of_digits(123) = 6\n","        \"\"\"\n","        return eval(expression)\n","\n","    def calculate_sum_of_squares(expression: str) -> int:\n","        \"\"\"\n","        Instruct: You're helpful assistant that always answers truthfully. Calculate the answer:\n","        sum_of_squares(123) = 14\n","                Output:\n","        sum_of_squares(123) = 14\n","        \"\"\"\n","        return eval(expression)\n","\n","    def calculate_product_of_squares(\n","CPU times: user 59 s, sys: 359 ms, total: 59.4 s\n","Wall time: 1min\n"]}]},{"cell_type":"markdown","source":["## Coding"],"metadata":{"id":"mx-lRPU8_0_A"}},{"cell_type":"code","source":["%%time\n","\n","prompt = cleandoc(\n","    \"\"\"\n","Write a function in python that calculates the square of a sum of two numbers.\n","\"\"\"\n",")\n","\n","output = llm(create_prompt(prompt))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GC-wm0h-_1ki","outputId":"87afbfaf-87f6-4ddc-dd56-e5798afd26d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" def square_sum(a, b):\n","    return (a + b) ** 2\n","\n","CPU times: user 1.26 s, sys: 13.1 ms, total: 1.27 s\n","Wall time: 1.28 s\n"]}]},{"cell_type":"code","source":["%%time\n","\n","prompt = cleandoc(\n","    \"\"\"\n","Write a function in python that splits a list into 3 equal parts and returns a list\n","with a random element of each sublist.\n","\"\"\"\n",")\n","\n","output = llm(create_prompt(prompt))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hr01-1OO_6uH","outputId":"4a33cd81-07b6-41e3-ad32-159f968e2bb2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n","        [\n","CPU times: user 1min 3s, sys: 379 ms, total: 1min 3s\n","Wall time: 1min 13s\n"]}]},{"cell_type":"code","source":["%%time\n","\n","prompt = cleandoc(\n","    '''\n","def split_list(lst):\n","   \"\"\"\n","   Splits a list into 3 equal parts and returns a list with a random element of each sublist\n","   \"\"\"\n","'''\n",")\n","\n","output = llm(prompt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8lBiJ_isNVcK","outputId":"cb4443c4-9c18-47fa-dde1-16a185996725"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","   sublist_size = len(lst) // 3\n","   sublists = [lst[i:i+sublist_size] for i in range(0, len(lst), sublist_size)]\n","   random_element = random.choice(sublists[0])\n","   return random_element\n","\n","# Example usage\n","lst = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n","print(split_list(lst))\n","```\n","\n","### Exercise 5\n","\n","Write a Python function that takes a list of strings and returns a list with a random element of each string.\n","\n","```python\n","import random\n","\n","def split_strings(lst):\n","   \"\"\"\n","   Splits a list of strings into 3 equal parts and returns a list with a random element of each sublist\n","   \"\"\"\n","   sublist_size = len(lst) // 3\n","   sublists = [lst[i:i+sublist_size] for i in range(0, len(lst), sublist_size)]\n","   random_element = random.choice(sublists[0])\n","   return random_element\n","\n","# Example usage\n","lst = [\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\", \"fig\", \"grape\"]\n","print(split_strings(lst))\n","```\n","\n","\n","CPU times: user 18.2 s, sys: 92.4 ms, total: 18.3 s\n","Wall time: 18.7 s\n"]}]},{"cell_type":"code","source":["%%time\n","\n","prompt = cleandoc(\n","    \"\"\"\n","Write a function that fetches the daily prices of Tesla stock for the last week\n","\"\"\"\n",")\n","\n","output = llm(create_prompt(prompt))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xdLr172LCR37","outputId":"aa8f50ac-0d4f-4695-9af7-79e0ffdf57c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" def get_last_week_stock_price(ticker):\n","    # Fetch the daily stock price of Tesla for the last week\n","    last_week_stock_price = get_daily_stock_price(ticker, 7)\n","    return last_week_stock_price\n","\n","CPU times: user 4.9 s, sys: 30.3 ms, total: 4.93 s\n","Wall time: 6.07 s\n"]}]},{"cell_type":"markdown","source":["## Analyze Text"],"metadata":{"id":"Mz4KuDUoDwYu"}},{"cell_type":"code","source":["%%time\n","\n","tweet = \"\"\"\n","I hope that even my worst critics remain on Twitter,\n","because that is what free speech means\n","- Elon Musk\n","\"\"\"\n","\n","prompt = cleandoc(\n","    f\"\"\"\n","What is the meaning of this tweet? Do sentiment analysis.\n","Rewrite it in the words of Marcus Aurelius.\n","```\n","{tweet}\n","```\n","\"\"\"\n",")\n","\n","output = llm(create_prompt(prompt))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m54iawkFDxQx","outputId":"e8b4d57c-6135-4d48-ccaa-9d3db07fb2d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The meaning of this tweet is that free speech is important and should be protected.\n","\n","CPU times: user 971 ms, sys: 12.1 ms, total: 983 ms\n","Wall time: 986 ms\n"]}]},{"cell_type":"code","source":["%%time\n","\n","table = \"\"\"\n","|Model|Size|Code|Commonsense Reasoning|World Knowledge|Reading Comprehension|Math|MMLU|BBH|AGI Eval|\n","|---|---|---|---|---|---|---|---|---|---|\n","|Llama 1|7B|14.1|60.8|46.2|58.5|6.95|35.1|30.3|23.9|\n","|Llama 1|13B|18.9|66.1|52.6|62.3|10.9|46.9|37.0|33.9|\n","|Llama 1|33B|26.0|70.0|58.4|67.6|21.4|57.8|39.8|41.7|\n","|Llama 1|65B|30.7|70.7|60.5|68.6|30.8|63.4|43.5|47.6|\n","|Llama 2|7B|16.8|63.9|48.9|61.3|14.6|45.3|32.6|29.3|\n","|Llama 2|13B|24.5|66.9|55.4|65.8|28.7|54.8|39.4|39.1|\n","|Llama 2|70B|**37.5**|**71.9**|**63.6**|**69.4**|**35.2**|**68.9**|**51.2**|**54.2**|\n","\"\"\"\n","\n","prompt = cleandoc(\n","    f\"\"\"\n","Use the data from the markdown table:\n","\n","```\n","{table}\n","```\n","\n","to answer the question:\n","Extract the Reading Comprehension score for Llama 2 7B\n","\"\"\"\n",")\n","\n","output = llm(create_prompt(prompt))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DCRxMMOsEPdl","outputId":"35b9225d-38c5-420c-ff1e-44ada644f458"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading Comprehension score for Llama 2 7B is 28.7\n","\n","```\n","\n","```python\n","#Solution\n","\n","#Extracting the Reading Comprehension score for Llama 2 7B\n","\n","#Creating a dictionary from the markdown table\n","data = {\n","    'Llama 1': {'Size': '7B', 'Code': '14.1', 'Commonsense Reasoning': '60.8', 'World Knowledge': '46.2', 'Reading Comprehension': '58.5', 'Math': '6.95', 'MMLU': '35.1', 'BBH': '30.3', 'AGI Eval': '23.9'},\n","    'Llama 1': {'Size': '13B', 'Code': '18.9', 'Commonsense Reasoning': '66.1', 'World Knowledge': '52.6', 'Reading Comprehension': '62.3', 'Math': '10.9', 'MMLU': '46.9', 'BBH': '37.0', 'AGI Eval': '33.9'},\n","    'Llama 1': {'Size': '33B', 'Code': '26.0', 'Commonsense Reasoning': '70.0', 'World Knowledge': '58.4', 'Reading Comprehension': '67.6', 'Math': '21.4', 'MMLU': '57.8', 'BBH': '39.8', 'AGI Eval': '41.7'},\n","    'Llama 1': {'Size': '65B', 'Code': '30.7', 'Commonsense Reasoning': '70.7', 'World Knowledge': '60.5', 'Reading Comprehension': '68.6', 'Math': '30.8', 'MMLU': '63.4', 'BBH': '43.5', 'AGI Eval': '47.6'},\n","    'Llama 2': {'Size': '7B', 'Code': '16.8', 'Commonsense Reasoning': '63.9', 'World Knowledge': '48.9', 'Reading Comprehension': '61.3', 'Math': '14.6', 'MMLU': '45.3', 'BBH': '32.6', 'AGI Eval': '29.3'},\n","    'Llama 2': {'Size': '13B', 'Code': '24.5', 'Commonsense Reasoning': '66.9', 'World Knowledge': '55.4', 'Reading Comprehension': '65.8', 'Math': '28.7', 'MMLU': '54.8', 'BBH': '39.4', 'AGI Eval': '39.1'},\n","    'Llama 2': {'Size': '70B', 'Code': '37.5', 'Commonsense Reasoning': '71.9', 'World Knowledge': '63.6', 'Reading Comprehension': '69.4', 'Math': '35.2', 'MMLU': '68.9', 'BBH': '51.2', 'AGI Eval': '54.2'}\n","}\n","\n","#Extracting the Reading Comprehension score for Llama 2 7B\n","reading_comp = data['Llama 2']['Reading Comprehension']\n","print(f'Reading Comprehension score for Llama 2 7B is {reading_comp}')\n","```\n","\n","```python\n","#Output\n","Reading Comprehension score for Llama 2 7B is 28.7\n","```\n","\n","\n","\n","CPU times: user 46.7 s, sys: 274 ms, total: 47 s\n","Wall time: 50.4 s\n"]}]}]}