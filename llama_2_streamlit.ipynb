{"cells":[{"cell_type":"markdown","metadata":{},"source":["<a href=\"https://colab.research.google.com/github/vvud/notebooks/blob/main/llama_2_streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t2voKBL-tmJ3"},"outputs":[],"source":["# Install necessary dependencies\n","!pip install transformers\n","!pip install ctransformers\n","!pip install streamlit\n","!pip install tensorflow\n","!pip install torch\n","!pip install bitsandbytes\n","!pip install accelerate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tgjI2j6E3eCM"},"outputs":[],"source":["!pip install huggingface_hub\n","from huggingface_hub import interpreter_login\n","interpreter_login()"]},{"cell_type":"markdown","metadata":{"id":"P4hnhIntzECA"},"source":["### Create file"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":309,"status":"ok","timestamp":1703842258993,"user":{"displayName":"Vuong Duc","userId":"09810728870389752084"},"user_tz":-420},"id":"2Ws2TzoWxUof","outputId":"92ea4c12-d140-44c3-af3c-57f9153f59c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing app.py\n"]}],"source":["%%writefile app.py\n","import streamlit as st\n","import streamlit as st\n","import os\n","from ctransformers import AutoModelForCausalLM\n","# from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# App title\n","st.set_page_config(page_title=\"ðŸ¦™ðŸ’¬ Llama 2 Chatbot\")\n","\n","PROMPT = \"You are a helpful assistant. You do not respond as 'User' or pretend to be 'User'. You only respond once as 'Assistant'.\"\n","# You are an AI Coding assistant. You do not respond as 'User' or pretend to be 'User'. You only respond once as 'AI Coding assistant'.\n","\n","@st.cache_resource()\n","def ChatModel(temperature, top_p):\n","    return AutoModelForCausalLM.from_pretrained(\n","        'TheBloke/Llama-2-7B-Chat-GGML',\n","        model_type='llama',\n","        temperature=temperature,\n","        top_p = top_p)\n","\n","#def ChatModel(temperature, top_p):\n","#    return AutoModelForCausalLM.from_pretrained(\n","#        'meta-llama/Llama-2-7b-chat-hf',\n","#        model_type='llama',\n","#        temperature=temperature,\n","#        top_p = top_p,\n","#        device_map='auto', load_in_8bit=True)\n","\n","# Custom st sidebar\n","with st.sidebar:\n","    st.title('ðŸ¦™ðŸ’¬ Llama 2 Chatbot')\n","    st.subheader('Models and parameters')\n","\n","    if 'prompt' not in st.session_state:\n","        st.session_state['prompt'] = PROMPT\n","\n","    #Model hyper parameters:\n","    temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=2.0, value=0.1, step=0.01)\n","    top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n","    # max_length = st.sidebar.slider('max_length', min_value=64, max_value=4096, value=2048, step=8)\n","\n","    PROMPT = st.sidebar.text_area('Prompt:', PROMPT, height=150)\n","\n","    chat_model = ChatModel(temperature, top_p)\n","    # st.markdown('ðŸ“– Learn how to build this app in this [blog](#link-to-blog)!')\n","\n","# Store LLM generated responses\n","if \"messages\" not in st.session_state.keys():\n","    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n","\n","# Display or clear chat messages\n","for message in st.session_state.messages:\n","    with st.chat_message(message[\"role\"]):\n","        st.write(message[\"content\"])\n","\n","def clear_chat_history():\n","    # st.session_state['chat_dialogue'] = []\n","    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n","st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n","\n","# Function for generating LLaMA2 response\n","def generate_llama2_response(prompt_input):\n","    string_dialogue = PROMPT\n","    for dict_message in st.session_state.messages:\n","        if dict_message[\"role\"] == \"user\":\n","            string_dialogue += \"User: \" + dict_message[\"content\"] + \"\\n\\n\"\n","        else:\n","            string_dialogue += \"Assistant: \" + dict_message[\"content\"] + \"\\n\\n\"\n","    print (string_dialogue)\n","    output = chat_model(f\"prompt {string_dialogue} {prompt_input} Assistant: \")\n","    return output\n","\n","# User-provided prompt\n","if prompt := st.chat_input():\n","    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n","    with st.chat_message(\"user\"):\n","        st.write(prompt)\n","\n","# Generate a new response if last message is not from assistant\n","if st.session_state.messages[-1][\"role\"] != \"assistant\":\n","    with st.chat_message(\"assistant\"):\n","        with st.spinner(\"Thinking...\"):\n","            response = generate_llama2_response(prompt)\n","            placeholder = st.empty()\n","            full_response = ''\n","            for item in response:\n","                full_response += item\n","                placeholder.markdown(full_response)\n","            placeholder.markdown(full_response)\n","    message = {\"role\": \"assistant\", \"content\": full_response}\n","    st.session_state.messages.append(message)\n"]},{"cell_type":"markdown","metadata":{"id":"WArgONrOzS-Z"},"source":["### Install localtunnel and run"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1081,"status":"ok","timestamp":1703842265546,"user":{"displayName":"Vuong Duc","userId":"09810728870389752084"},"user_tz":-420},"id":"uu6QPVzVvpx_","outputId":"f4e06ac8-8390-47a3-b9fd-deb2f1cfeee3"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n","\u001b[0m\n","+ localtunnel@2.0.2\n","updated 1 package and audited 36 packages in 0.42s\n","\n","3 packages are looking for funding\n","  run `npm fund` for details\n","\n","found 2 \u001b[93mmoderate\u001b[0m severity vulnerabilities\n","  run `npm audit fix` to fix them, or `npm audit` for details\n","\u001b[K\u001b[?25h"]}],"source":["!npm install localtunnel"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":290,"status":"ok","timestamp":1703842275741,"user":{"displayName":"Vuong Duc","userId":"09810728870389752084"},"user_tz":-420},"id":"eylRBATUw4-8"},"outputs":[],"source":["!streamlit run /content/app.py &> /content/logs.txt &"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":316,"status":"ok","timestamp":1703842279074,"user":{"displayName":"Vuong Duc","userId":"09810728870389752084"},"user_tz":-420},"id":"8d4PWgsH2Nao","outputId":"bf37a854-479f-411d-b034-94531ebe0ffd"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n","\n","\n","  You can now view your Streamlit app in your browser.\n","\n","  Network URL: http://172.28.0.12:8501\n","  External URL: http://34.173.73.245:8501\n","\n","\n"]}],"source":["path = '/content/logs.txt'\n","with open(path, 'r') as f:\n","  print(f.read())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XRpPk0cKwP2T","outputId":"759f95bf-26ea-45b6-ff60-444647279ccf"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[K\u001b[?25hnpx: installed 22 in 2.28s\n","your url is: https://cool-birds-hug.loca.lt\n"]}],"source":["!npx localtunnel --port 8501"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPdcMAjkiSL9xArdVqhmnu2","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
